{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD4yL9qVB-tO",
        "outputId": "c94f9c07-c9ca-431c-87af-785feb5bbda5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFdAUA7m-zgS",
        "outputId": "5bef1867-a1dd-4750-8f0f-b121efe37a5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class JobSimilarityAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def tokenize_and_filter(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        filtered_tokens = [word.lower() for word in tokens if word.lower() not in self.stop_words]\n",
        "\n",
        "        pos_tags = pos_tag(filtered_tokens)\n",
        "        filtered_words = [word for word, pos in pos_tags if pos in ['NNP', 'NN']]\n",
        "\n",
        "        if not len(filtered_words) > 1:\n",
        "            raise ValueError(\"The length is wrong\")\n",
        "\n",
        "        return ' '.join(filtered_words)\n",
        "\n",
        "    def get_cosine_similarity(self, text1, text2):\n",
        "        vectorizer = CountVectorizer()\n",
        "        matrix = vectorizer.fit_transform([text1, text2])\n",
        "        similarity = cosine_similarity(matrix)\n",
        "        return similarity[0][1]\n",
        "\n",
        "    def process_job_files(self, file_paths):\n",
        "        job_descriptions = []\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            with open(\"/content/drive/MyDrive/Colab Notebooks/jobs/\" + file_path, 'r') as file:\n",
        "                text = file.read()\n",
        "                filtered_tokens = self.tokenize_and_filter(text)\n",
        "                job_descriptions.append([file_path, filtered_tokens])\n",
        "\n",
        "        return job_descriptions\n",
        "\n",
        "    def calculate_similarity_scores(self, job_descriptions, user_text):\n",
        "        user_text_tokens = self.tokenize_and_filter(user_text)\n",
        "        similarity_scores = []\n",
        "\n",
        "        for job in job_descriptions:\n",
        "            similarity = self.get_cosine_similarity(user_text_tokens, job[1])\n",
        "            similarity_scores.append((job[0], similarity))\n",
        "\n",
        "        similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return similarity_scores\n"
      ],
      "metadata": {
        "id": "atFiZaNqE5iq"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_sentence = \"\"\"As a highly skilled and motivated data engineering professional, I possess a robust foundation in database design, having successfully implemented and optimized scalable database solutions using technologies such as SQL, NoSQL, and cloud-based data storage systems. My proficiency extends to ETL (Extract, Transform, Load) processes, where I have demonstrated expertise in designing and implementing efficient data pipelines to seamlessly integrate and transform diverse data sources, ensuring data quality and integrity.\n",
        "\n",
        "I am adept at utilizing programming languages such as Python and Java to develop custom data processing scripts and applications, automating data workflows, and enhancing overall system efficiency. My experience includes working with big data technologies, including Hadoop and Spark, enabling me to process and analyze large datasets with a focus on performance optimization.\n",
        "\n",
        "In addition, I have a strong understanding of data warehousing concepts and have effectively designed and maintained data warehouses, providing stakeholders with timely and accurate business intelligence. My experience with data modeling and schema design contributes to creating well-organized and accessible data structures that cater to specific business requirements.\n",
        "\n",
        "Furthermore, I am well-versed in cloud platforms such as AWS, Azure, and Google Cloud, utilizing their services for data storage, processing, and analytics. My expertise extends to version control systems, ensuring proper documentation and collaborative development practices. As a proactive problem solver, I am accustomed to troubleshooting and resolving complex data issues, ensuring the continuous flow and availability of high-quality data for analytical purposes.\n",
        "\n",
        "With a keen eye for emerging trends in data engineering and a commitment to staying abreast of technological advancements, I am confident in my ability to contribute effectively to any data engineering team, driving innovation and delivering robust solutions to meet the dynamic needs of the organization.\"\"\"\n",
        "\n",
        "\n",
        "analyzer = JobSimilarityAnalyzer()\n",
        "\n",
        "file_paths = [ \"data_engineer.txt\", \"backend_developer.txt\", \"frontend_developer.txt\", \"sales_manager.txt\"]\n",
        "\n",
        "# Process job files and calculate similarity scores\n",
        "job_descriptions = analyzer.process_job_files(file_paths)\n",
        "similarity_scores = analyzer.calculate_similarity_scores(job_descriptions, test_sentence)\n",
        "\n",
        "# Print similarity scores\n",
        "print(\"You most likely want to do this jobs: \")\n",
        "for idx1, similarity in similarity_scores:\n",
        "    print(f\"{idx1}: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ditFa-qIF7q0",
        "outputId": "b48c0159-82b6-4867-de63-cfe867855e35"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You most likely want to do this jobs: \n",
            "data_engineer.txt: 0.3828478769486491\n",
            "frontend_developer.txt: 0.27183741892777974\n",
            "backend_developer.txt: 0.18072289156626506\n",
            "sales_manager.txt: 0.14065447086831775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B6JOcNP2SmYd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}